\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\title{LA-HW5}
\author{Zahra Rafati}
\date{\today}
\begin{document}
\maketitle

\section*{Question 1}
If we represent the eigenvalues of $A$ with $\lambda$, then the eigenvalues of $A - \alpha I$ are $\lambda - \alpha$.

The eigenvectors of $A$ are also eigenvectors of $A - \alpha I$:
\[
A\mathbf{v} = \lambda\mathbf{v} \implies (A - \alpha I)\mathbf{v} = A\mathbf{v} - \alpha I\mathbf{v}
\]
\[
= \lambda\mathbf{v} - \alpha\mathbf{v} = (\lambda - \alpha)\mathbf{v}.
\]


\section*{Question 2}
The characteristic polynomial of $A$ is defined as:
\[
p_A(\lambda) = \det(A - \lambda I).
\]

Similarly, the characteristic polynomial of $A^T$ is:
\[
p_{A^T}(\lambda) = \det(A^T - \lambda I).
\]

Now, consider the determinant of $A^T - \lambda I$:
\[
\det(A^T - \lambda I) = \det((A - \lambda I)^T) = \det(A - \lambda I).
\]

This implies:
\[
p_{A^T}(\lambda) = p_A(\lambda).
\]

Therefore, the eigenvalues of $A$ are the same as the eigenvalues of $A^T$.


\section*{Question 3}
Let $A$ be a stochastic matrix. This implies:
\[
A_{ij} \geq 0 \quad \text{and} \quad \sum_i A_{ij} = 1 \quad \text{for all } j.
\]

Similarly, for $A^T$, we have:
\[
\sum_j A_{ij}^T = \sum_j A_{ji} = 1 \quad \text{for all } i,
\]
which implies that $A^T$ is also stochastic.

If we consider the vector $\mathbf{1} = [1, 1, \dots, 1]^T$, then:
\[
A^T \mathbf{1} = \mathbf{1}.
\]

This shows that $\mathbf{1}$ is an eigenvector of $A^T$ corresponding to the eigenvalue $\lambda = 1$.

From Question 2, we conclude that the eigenvalues of $A$ and $A^T$ are the same.


\section*{Question 4}
\subsection*{(a)}
Given:
\[
A\mathbf{v} = \lambda\mathbf{v}.
\]

We know that $A\mathbf{v}$ is in the column space of $A$, and $\lambda\mathbf{v}$ is also in the column space of $A$. Since $\lambda$ is a nonzero scalar, $\mathbf{v}$ is in the column space of $A$.

\subsection*{(b)}
Let $\mathbf{v} = \mathbf{v}_r + \mathbf{v}_n$, where $\mathbf{v}_r$ is in the row space of $A$ and $\mathbf{v}_n$ is in the null space of $A$. Then:
\[
A\mathbf{v} = A(\mathbf{v}_r + \mathbf{v}_n) = A\mathbf{v}_r + A\mathbf{v}_n.
\]

Since $\mathbf{v}_n$ is in the null space of $A$, we have $A\mathbf{v}_n = \mathbf{0}$. Therefore:
\[
A\mathbf{v} = A\mathbf{v}_r + \mathbf{0} = A\mathbf{v}_r.
\]

From part (a), we concluded that $\mathbf{v}$ is in the column space of $A$. Hence:
\[
A\mathbf{v} = \lambda\mathbf{v} \neq \mathbf{0}.
\]

This implies:
\[
A\mathbf{v}_r \neq \mathbf{0}, \quad \text{so } \mathbf{v}_r \neq \mathbf{0}.
\]


\section*{Question 5}
Given:
\[
A\mathbf{v}_i = \lambda_i\mathbf{v}_i \quad \text{and} \quad A\mathbf{v}_j = \lambda_j\mathbf{v}_j.
\]

Consider the inner product $\mathbf{v}_j^T (A\mathbf{v}_i)$:
\[
\mathbf{v}_j^T (A\mathbf{v}_i) = \mathbf{v}_j^T (\lambda_i \mathbf{v}_i) = \lambda_i (\mathbf{v}_j^T \mathbf{v}_i).
\]

Since $A$ is symmetric, $A = A^T$. Therefore:
\[
\mathbf{v}_j^T (A\mathbf{v}_i) = (A\mathbf{v}_j)^T \mathbf{v}_i = (\lambda_j \mathbf{v}_j)^T \mathbf{v}_i = \lambda_j (\mathbf{v}_j^T \mathbf{v}_i).
\]

Equating the two expressions:
\[
\lambda_i (\mathbf{v}_j^T \mathbf{v}_i) = \lambda_j (\mathbf{v}_j^T \mathbf{v}_i).
\]

This implies:
\[
(\lambda_i - \lambda_j)(\mathbf{v}_j^T \mathbf{v}_i) = 0.
\]

If $\lambda_i \neq \lambda_j$, then:
\[
\mathbf{v}_j^T \mathbf{v}_i = 0.
\]

Thus, $\mathbf{v}_i$ and $\mathbf{v}_j$ are orthogonal when $\lambda_i \neq \lambda_j$.


\section*{Question 6}
A matrix $A$ is positive definite if:
\[
\mathbf{x}^T A \mathbf{x} > 0 \quad \text{for all nonzero vectors } \mathbf{x}.
\]

If $A = V \Lambda V^T$, then:
\[
\mathbf{x}^T A \mathbf{x} = \mathbf{x}^T (V \Lambda V^T) \mathbf{x}.
\]

Let $\mathbf{y} = V^T \mathbf{x}$, then:
\[
\mathbf{x}^T A \mathbf{x} = \mathbf{y}^T \Lambda \mathbf{y} = \lambda_1 y_1^2 + \lambda_2 y_2^2 + \cdots + \lambda_n y_n^2.
\]

Since $\mathbf{x}^T A \mathbf{x} > 0$ for all nonzero $\mathbf{x}$:
\[
\sum_{i=1}^n \lambda_i y_i^2 > 0 \quad \text{for all nonzero } \mathbf{y}.
\]

This implies that all $\lambda_i > 0$. (If any $\lambda_i$ were negative, the equation would not hold as the sum could become negative.)

Thus:
\[
\text{If } A \text{ is positive definite, then all eigenvalues of } A \text{ are positive.}
\]

Conversely, if all $\lambda_i > 0$, then:
\[
\mathbf{x}^T A \mathbf{x} = \mathbf{y}^T \Lambda \mathbf{y} = \lambda_1 y_1^2 + \lambda_2 y_2^2 + \cdots + \lambda_n y_n^2 > 0.
\]

Hence:
\[
\text{If all eigenvalues of } A \text{ are positive, then } A \text{ is positive definite.}
\]


\section*{Question 7}
If $A$ is positive definite, then:
\[
\mathbf{x}^T A \mathbf{x} > 0 \quad \text{for all nonzero vectors } \mathbf{x}.
\]

Let $\mathbf{x} = \mathbf{e}_i$, where $\mathbf{e}_i$ is the $i$-th standard basis vector. Then:
\[
\mathbf{e}_i^T A \mathbf{e}_i = A_{ii}.
\]

Since $\mathbf{x}^T A \mathbf{x} > 0$ for all nonzero $\mathbf{x}$:
\[
\mathbf{e}_i^T A \mathbf{e}_i > 0 \implies A_{ii} > 0.
\]

Thus, the diagonal entries of a positive definite matrix $A$ are positive.


\section*{Question 8}
\subsection*{(a)}
For all $\mathbf{u} \in \mathbb{R}^n$, we have:
\[
\langle \mathbf{u}, \mathbf{u} \rangle_A > 0.
\]

Since $A$ is positive definite:
\[
\mathbf{u}^T A \mathbf{u} > 0 \quad \text{for all nonzero } \mathbf{u},
\]
and:
\[
\mathbf{u}^T A \mathbf{u} = 0 \quad \text{if } \mathbf{u} = 0.
\]

\subsection*{(b)}
$\langle \mathbf{u}, \mathbf{u} \rangle_A = 0$ if and only if $\mathbf{u} = 0$. 

From part (a), we conclude that the statement is true.

\subsection*{(c)}
The bilinear form $\langle \mathbf{u}, \mathbf{v} \rangle_A$ satisfies:
\[
\langle \mathbf{u}, \mathbf{v} \rangle_A = \langle \mathbf{v}, \mathbf{u} \rangle_A.
\]

Since $A$ is symmetric ($A = A^T$):
\[
\mathbf{u}^T A \mathbf{v} = (\mathbf{v}^T A \mathbf{u})^T = \mathbf{v}^T A \mathbf{u}.
\]

\subsection*{(d)}
The bilinear form is linear in its arguments:
\[
\langle \alpha \mathbf{u} + \beta \mathbf{v}, \mathbf{w} \rangle_A = (\alpha \mathbf{u} + \beta \mathbf{v})^T A \mathbf{w}.
\]

Expanding this, we get:
\[
\langle \alpha \mathbf{u} + \beta \mathbf{v}, \mathbf{w} \rangle_A = \alpha (\mathbf{u}^T A \mathbf{w}) + \beta (\mathbf{v}^T A \mathbf{w}),
\]
which simplifies to:
\[
\langle \alpha \mathbf{u} + \beta \mathbf{v}, \mathbf{w} \rangle_A = \alpha \langle \mathbf{u}, \mathbf{w} \rangle_A + \beta \langle \mathbf{v}, \mathbf{w} \rangle_A.
\]


\end{document}
